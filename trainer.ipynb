{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yyvuZskjWgp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.v2\n",
        "from pytorchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import utils\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from torch import nn, Tensor\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from multiprocessing import Pool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PianoModelBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, pad=True, ksize=(3, 3), stride=(1, 1), drop=0.0):\n",
        "        super().__init__()\n",
        "        padding = (ksize[0] // 2, ksize[1] // 2) if pad else (0, 0)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_dim, out_dim, kernel_size=ksize, stride=stride, bias=False, padding=padding),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Dropout(p=drop),\n",
        "\n",
        "            nn.Conv2d(out_dim, out_dim, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "        )\n",
        "\n",
        "        self.relu = nn.LeakyReLU(inplace=True)\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, out_dim, kernel_size=ksize, stride=stride, bias=False, padding=padding),\n",
        "            nn.BatchNorm2d(out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x = self.relu(self.main(x) + self.downsample(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PianoModelSmallSelf(nn.Module):\n",
        "    def __init__(self, input_size=(480, 640), k=5) -> None:\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.dim_after_preprocessing = 16\n",
        "        downscale_dim_sizes = [32, 32, 64, 128, 128, 256]\n",
        "\n",
        "        self.agg_input_frames = nn.Sequential(\n",
        "            nn.Conv3d(1, 8, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(8),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "\n",
        "            nn.Conv3d(8, 16, kernel_size=(3, 3, 3), padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "\n",
        "            nn.Conv3d(16, downscale_dim_sizes[0], kernel_size=(self.k - 2, 3, 3), padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(downscale_dim_sizes[0]),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.pred_frame_conv = PianoModelBlock(in_dim=1, out_dim=downscale_dim_sizes[0], ksize=(3, 3), stride=1)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            PianoModelBlock(in_dim=downscale_dim_sizes[0], out_dim=downscale_dim_sizes[1], ksize=(3, 3), stride=2, drop=0.2),\n",
        "            PianoModelBlock(in_dim=downscale_dim_sizes[1], out_dim=downscale_dim_sizes[2], ksize=(3, 3), stride=2, drop=0.2),\n",
        "            PianoModelBlock(in_dim=downscale_dim_sizes[2], out_dim=downscale_dim_sizes[3], ksize=(3, 3), stride=(2, 1), drop=0.2),\n",
        "            PianoModelBlock(in_dim=downscale_dim_sizes[3], out_dim=downscale_dim_sizes[4], ksize=(3, 3), stride=(2, 1), drop=0.2),\n",
        "            PianoModelBlock(in_dim=downscale_dim_sizes[4], out_dim=downscale_dim_sizes[5], ksize=(3, 3), stride=(2, 1), drop=0.0),\n",
        "        ])\n",
        "        # self.avgpool = nn.AdaptiveAvgPool2d((1, 88))\n",
        "        self.fc1 = nn.Linear(input_size[0] // 32, 1)\n",
        "        final_conv_dim = 256\n",
        "        self.final_conv = nn.Conv1d(downscale_dim_sizes[-1], final_conv_dim, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(input_size[1] // 4 * final_conv_dim, 88)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        pred_frame = self.pred_frame_conv(x[:, :, -1])\n",
        "        x = self.agg_input_frames(x)\n",
        "\n",
        "        x = x[:, :, 0] + pred_frame\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = torch.moveaxis(x, 2, 3)\n",
        "        x = self.fc1(x)\n",
        "        x = x[:, :, :, 0]\n",
        "        # x = self.avgpool(x)\n",
        "        x = self.final_conv(x)\n",
        "        x = self.fc(x.flatten(1))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# k = 7\n",
        "# inpt = torch.rand(size=[1, 1, k, 480, 640])\n",
        "# model = PianoModelSmallSelf(k=k)\n",
        "# #model = PianoModel()\n",
        "# print(summary(input_size=inpt.squeeze(0).shape, model=model))\n",
        "# print(\"in shape:\", inpt.shape)\n",
        "# print(\"out shape:\", model(inpt).shape)\n"
      ],
      "metadata": {
        "id": "8HV1J4_kjwxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames = [os.path.join(\"data\", a) for a in os.listdir(\"data\") if \".png\" in a]\n",
        "fnames += [os.path.join(\"data2\", a) for a in os.listdir(\"data2\") if \".png\" in a]\n",
        "fnames += [os.path.join(\"data3\", a) for a in os.listdir(\"data3\") if \".png\" in a]\n",
        "fnames += [os.path.join(\"data4\", a) for a in os.listdir(\"data4\") if \".png\" in a]\n",
        "fnames += [os.path.join(\"data5\", a) for a in os.listdir(\"data5\") if \".png\" in a]\n",
        "fnames = sorted(fnames)\n",
        "img_fnames = [fname for fname in fnames]\n",
        "annot_fnames = [fname.replace(\".png\", \".npy\") for fname in fnames]\n",
        "\n",
        "for a in annot_fnames:\n",
        "  assert os.path.exists(a), f\"{a}\"\n",
        "\n",
        "pbar = tqdm(total=len(fnames))\n",
        "def f1(img_fname):\n",
        "  pbar.update(1)\n",
        "  return cv2.imread(img_fname, cv2.IMREAD_GRAYSCALE)\n",
        "with ThreadPool(64) as p:\n",
        "  imgs = p.map(f1, img_fnames)\n",
        "pbar.close()\n",
        "\n",
        "pbar = tqdm(total=len(fnames))\n",
        "def f2(label_fname):\n",
        "  pbar.update(1)\n",
        "  return np.load(label_fname)\n",
        "with ThreadPool(64) as p:\n",
        "  labels = p.map(f2, annot_fnames)\n",
        "pbar.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E2lf-23ikQmx",
        "outputId": "dd5df992-08e0-448e-a47e-bf6cdc3bd34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████| 55557/55557 [02:35<00:00, 356.90it/s]\n",
            "100%|████████████████████████████████| 55557/55557 [00:40<00:00, 1385.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PianoDataset(Dataset):\n",
        "    def __init__(self, idxs=None, transform=None, k=5):\n",
        "        self.transform = transform\n",
        "        self.idxs = idxs if idxs is not None else np.arange(len(img_fnames))\n",
        "        self.k = k\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = self.idxs[idx]\n",
        "        image = torch.cat([torch.tensor(imgs[idx + (i - self.k)] / 255, dtype=torch.float32, device=\"cpu\").unsqueeze(0).unsqueeze(0) for i in range(1, self.k + 1)], dim=1)\n",
        "        label = torch.tensor(labels[idx], dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image.permute(1, 0, 2, 3)).permute(1, 0, 2, 3)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "8pSru1-Ljlq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, val_loader, device, threshhold=0.5):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            outputs = F.sigmoid(model(batch[0].to(device))) > threshhold\n",
        "            accs.append((outputs == batch[1].to(device)).to(torch.float32).mean().item())\n",
        "    del batch\n",
        "    utils.empty_cache()\n",
        "    return np.array(accs).mean()\n",
        "\n",
        "def loss_fn(pred, truth):\n",
        "    weights = torch.full_like(pred, fill_value=1/10, device=device)\n",
        "    weights[truth > 0] = 9/10\n",
        "    return F.binary_cross_entropy_with_logits(pred, truth, weight=weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "TG_kW9-kkqKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.v2.ColorJitter(brightness=.4, saturation=.4, hue=.5, contrast=.3),\n",
        "    torchvision.transforms.v2.Lambda(lambda image: F.dropout(image, p=0.05, inplace=True)),\n",
        "    torchvision.transforms.v2.RandomAffine(degrees=5, translate=(0.05, 0.4)),  # 5, (0, 0.4)\n",
        "    torchvision.transforms.v2.RandomPerspective(distortion_scale=0.3, p=0.9),  # p=1\n",
        "])\n",
        "\n",
        "idxs = np.arange(len(PianoDataset()))\n",
        "np.random.shuffle(idxs)\n",
        "num_last_images = 5\n",
        "train_split = 0.9\n",
        "train_set = PianoDataset(transform=transform, idxs=idxs[:int(len(idxs) * train_split)], k=num_last_images)\n",
        "val_set = PianoDataset(idxs=idxs[int(len(idxs) * train_split):], k=num_last_images)\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "train_dl = DataLoader(train_set,\n",
        "                      batch_size=40,#utils.find_batch_size(model, dataset=train_set, max_val=16, is_train=True, shuffle=True, device=device),\n",
        "                      shuffle=True,\n",
        "                      num_workers=12,\n",
        "                      )\n",
        "val_dl = DataLoader(val_set,\n",
        "                    batch_size=128,#utils.find_batch_size(model, dataset=val_set, max_val=128, is_train=False, shuffle=False, device=device),\n",
        "                    num_workers=8,\n",
        "                    )\n",
        "\n",
        "print(f\"batch sizes: ({train_dl.batch_size}, {val_dl.batch_size}), train len: {len(train_dl)}, val len: {len(val_dl)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HNfcs_jktWo",
        "outputId": "2e2516b2-68e9-411f-eede-5659ed7bb75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch sizes: (40, 128), train len: 1251, val len: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/gpfs/space/home/karlraud/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PianoModelSmallSelf(k=num_last_images)\n",
        "model.load_state_dict(torch.load(\"models/model_e14_0.9933\"), strict=True)\n",
        "model = model.to(device).train()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 200\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_dl))\n",
        "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)"
      ],
      "metadata": {
        "id": "8LL8LewhzeEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_val_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nepoch {epoch+1}/{epochs}\")\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    model.train()\n",
        "    iterator = tqdm(enumerate(train_dl), total=len(train_dl))\n",
        "    for i, batch in iterator:\n",
        "        if i == len(train_dl) - 1:  # skip last batch\n",
        "          continue\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        X, y = batch[0].to(device), batch[1].to(device)\n",
        "        outputs = model(X)\n",
        "        loss = loss_fn(outputs, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_acc += ((F.sigmoid(outputs) > 0.5) == y).to(torch.float32).mean().item()\n",
        "        total_loss += loss.item()\n",
        "        used, total = utils.get_vram_usage()\n",
        "        iterator.set_description(\n",
        "            f\"vram: {used:.2f}G/{total:.2f}G \"\n",
        "            f\"ram: {utils.get_ram_usage():.2f} \"\n",
        "            f\"lr: {optimizer.param_groups[0]['lr']:.8f} \"\n",
        "            f\"loss: {total_loss / (i+1):.5f} \"\n",
        "            f\"acc: {total_acc / (i+1):.5f} \"\n",
        "            f\"val acc: {last_val_acc:.4f} \"\n",
        "        )\n",
        "\n",
        "    del batch\n",
        "    del outputs\n",
        "    utils.empty_cache()\n",
        "\n",
        "    last_val_acc = val(model, val_dl, device, threshhold=0.5)\n",
        "    torch.save(model.state_dict(), f\"models/model_e{epoch+1}_{last_val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr9xUaaJk8rL",
        "outputId": "0ddaa94a-8774-4866-de5b-5b4dae04f566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 53.02G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00569 acc: 0.98968 val acc: 0.0000 : 100%|█████████████████████████| 1251/1251 [21:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 2/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.70G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00540 acc: 0.99016 val acc: 0.9918 : 100%|█████████████████████████| 1251/1251 [21:40<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 3/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00524 acc: 0.99040 val acc: 0.9930 : 100%|█████████████████████████| 1251/1251 [21:53<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 4/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00516 acc: 0.99052 val acc: 0.9930 : 100%|█████████████████████████| 1251/1251 [21:35<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 5/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00502 acc: 0.99071 val acc: 0.9933 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 6/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00503 acc: 0.99080 val acc: 0.9936 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 7/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00494 acc: 0.99089 val acc: 0.9924 : 100%|█████████████████████████| 1251/1251 [21:38<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 8/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00489 acc: 0.99097 val acc: 0.9924 : 100%|█████████████████████████| 1251/1251 [21:37<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 9/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00480 acc: 0.99114 val acc: 0.9930 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 10/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00478 acc: 0.99111 val acc: 0.9929 : 100%|█████████████████████████| 1251/1251 [21:45<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 11/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00474 acc: 0.99120 val acc: 0.9917 : 100%|█████████████████████████| 1251/1251 [21:36<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 12/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00471 acc: 0.99129 val acc: 0.9910 : 100%|█████████████████████████| 1251/1251 [21:41<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 13/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00464 acc: 0.99136 val acc: 0.9929 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 14/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00463 acc: 0.99141 val acc: 0.9915 : 100%|█████████████████████████| 1251/1251 [21:34<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 15/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00460 acc: 0.99144 val acc: 0.9928 : 100%|█████████████████████████| 1251/1251 [21:40<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 16/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00457 acc: 0.99147 val acc: 0.9924 : 100%|█████████████████████████| 1251/1251 [21:30<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 17/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00451 acc: 0.99157 val acc: 0.9925 : 100%|█████████████████████████| 1251/1251 [21:28<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 18/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00448 acc: 0.99160 val acc: 0.9929 : 100%|█████████████████████████| 1251/1251 [21:42<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 19/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00451 acc: 0.99163 val acc: 0.9935 : 100%|█████████████████████████| 1251/1251 [21:35<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 20/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00445 acc: 0.99163 val acc: 0.9937 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 21/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00438 acc: 0.99179 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:45<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 22/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00436 acc: 0.99181 val acc: 0.9940 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 23/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00435 acc: 0.99187 val acc: 0.9933 : 100%|█████████████████████████| 1251/1251 [21:37<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 24/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00435 acc: 0.99187 val acc: 0.9930 : 100%|█████████████████████████| 1251/1251 [21:32<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 25/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00429 acc: 0.99196 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:32<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 26/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00428 acc: 0.99198 val acc: 0.9935 : 100%|█████████████████████████| 1251/1251 [21:50<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 27/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00429 acc: 0.99196 val acc: 0.9927 : 100%|█████████████████████████| 1251/1251 [21:34<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 28/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00421 acc: 0.99207 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:31<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 29/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00422 acc: 0.99209 val acc: 0.9932 : 100%|█████████████████████████| 1251/1251 [21:34<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 30/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00416 acc: 0.99213 val acc: 0.9938 : 100%|█████████████████████████| 1251/1251 [21:37<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 31/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00411 acc: 0.99227 val acc: 0.9941 : 100%|█████████████████████████| 1251/1251 [21:55<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 32/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00415 acc: 0.99214 val acc: 0.9942 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 33/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00409 acc: 0.99224 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:35<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 34/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00409 acc: 0.99230 val acc: 0.9942 : 100%|█████████████████████████| 1251/1251 [21:35<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 35/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00404 acc: 0.99235 val acc: 0.9932 : 100%|█████████████████████████| 1251/1251 [21:40<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 36/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00404 acc: 0.99236 val acc: 0.9922 : 100%|█████████████████████████| 1251/1251 [21:45<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 37/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00407 acc: 0.99233 val acc: 0.9927 : 100%|█████████████████████████| 1251/1251 [21:40<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 38/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00399 acc: 0.99244 val acc: 0.9939 : 100%|█████████████████████████| 1251/1251 [21:42<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 39/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00398 acc: 0.99246 val acc: 0.9930 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 40/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00393 acc: 0.99255 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:31<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 41/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00396 acc: 0.99246 val acc: 0.9929 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 42/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00396 acc: 0.99258 val acc: 0.9943 : 100%|█████████████████████████| 1251/1251 [21:45<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 43/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00390 acc: 0.99260 val acc: 0.9920 : 100%|█████████████████████████| 1251/1251 [21:34<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 44/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00389 acc: 0.99263 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 45/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00388 acc: 0.99263 val acc: 0.9945 : 100%|█████████████████████████| 1251/1251 [21:50<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 46/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00385 acc: 0.99266 val acc: 0.9948 : 100%|█████████████████████████| 1251/1251 [21:39<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 47/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00381 acc: 0.99276 val acc: 0.9936 : 100%|█████████████████████████| 1251/1251 [21:35<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 48/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00380 acc: 0.99274 val acc: 0.9935 : 100%|█████████████████████████| 1251/1251 [21:42<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 49/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00382 acc: 0.99275 val acc: 0.9942 : 100%|█████████████████████████| 1251/1251 [21:52<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 50/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00380 acc: 0.99274 val acc: 0.9948 : 100%|█████████████████████████| 1251/1251 [21:34<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 51/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00379 acc: 0.99282 val acc: 0.9931 : 100%|█████████████████████████| 1251/1251 [21:36<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 52/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00376 acc: 0.99281 val acc: 0.9937 : 100%|█████████████████████████| 1251/1251 [21:38<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 53/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00375 acc: 0.99283 val acc: 0.9932 : 100%|█████████████████████████| 1251/1251 [21:25<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 54/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00371 acc: 0.99289 val acc: 0.9946 : 100%|█████████████████████████| 1251/1251 [21:31<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 55/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00370 acc: 0.99288 val acc: 0.9932 : 100%|█████████████████████████| 1251/1251 [21:41<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 56/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00371 acc: 0.99289 val acc: 0.9934 : 100%|█████████████████████████| 1251/1251 [21:26<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 57/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00366 acc: 0.99300 val acc: 0.9946 : 100%|█████████████████████████| 1251/1251 [21:24<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 58/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00369 acc: 0.99298 val acc: 0.9937 : 100%|█████████████████████████| 1251/1251 [21:19<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 59/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00367 acc: 0.99300 val acc: 0.9940 : 100%|█████████████████████████| 1251/1251 [21:24<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 60/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00362 acc: 0.99309 val acc: 0.9943 : 100%|█████████████████████████| 1251/1251 [21:47<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 61/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00364 acc: 0.99301 val acc: 0.9947 : 100%|█████████████████████████| 1251/1251 [21:30<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 62/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00364 acc: 0.99302 val acc: 0.9941 : 100%|█████████████████████████| 1251/1251 [21:27<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 63/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00359 acc: 0.99313 val acc: 0.9939 : 100%|█████████████████████████| 1251/1251 [21:25<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 64/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00358 acc: 0.99315 val acc: 0.9935 : 100%|█████████████████████████| 1251/1251 [21:19<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 65/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00352 acc: 0.99328 val acc: 0.9939 : 100%|█████████████████████████| 1251/1251 [21:41<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 66/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00355 acc: 0.99320 val acc: 0.9944 : 100%|█████████████████████████| 1251/1251 [21:25<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 67/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00354 acc: 0.99322 val acc: 0.9932 : 100%|█████████████████████████| 1251/1251 [21:31<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 68/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.06 lr: 0.00100000 loss: 0.00357 acc: 0.99314 val acc: 0.9941 : 100%|█████████████████████████| 1251/1251 [21:33<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 69/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.09 lr: 0.00100000 loss: 0.00352 acc: 0.99326 val acc: 0.9947 : 100%|█████████████████████████| 1251/1251 [24:39<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 70/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.07 lr: 0.00100000 loss: 0.00349 acc: 0.99327 val acc: 0.9942 : 100%|█████████████████████████| 1251/1251 [24:58<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 71/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vram: 54.67G/84.99G ram: 0.10 lr: 0.00100000 loss: 0.00351 acc: 0.99327 val acc: 0.9945 :  67%|█████████████████▎        | 832/1251 [18:28<09:30,  1.36s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PianoModelSmallSelf(k=num_last_images)\n",
        "model.load_state_dict(torch.load(\"models/model_e1_0.9953\"), strict=False)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "\n",
        "threshholds = np.arange(0, 1.01, 0.01)\n",
        "accs = dict()\n",
        "with torch.no_grad():\n",
        "    for i, batch in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
        "        outputs = F.sigmoid(model(batch[0].to(device)))\n",
        "        l = batch[1].to(device)\n",
        "        for thresh in threshholds:\n",
        "          o = outputs > thresh\n",
        "          accs[thresh] = accs.get(thresh, []) + [(o == l).to(torch.float32).mean().item()]\n",
        "del batch\n",
        "utils.empty_cache()\n",
        "for x, v in accs.items():\n",
        "  accs[x] = np.array(v).mean()\n",
        "for x, v in accs.items():\n",
        "  print(f\"{x:.2f}: {v:.5f}\" + (\" best\" if v == max(accs.values()) else \"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOvw5Jm4k93W",
        "outputId": "7e7a62ea-bb07-4a9c-9af8-3288ddd77917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:37<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00: 0.02240\n",
            "0.01: 0.97655\n",
            "0.02: 0.98506\n",
            "0.03: 0.98852\n",
            "0.04: 0.99029\n",
            "0.05: 0.99139\n",
            "0.06: 0.99223\n",
            "0.07: 0.99284\n",
            "0.08: 0.99329\n",
            "0.09: 0.99360\n",
            "0.10: 0.99386\n",
            "0.11: 0.99406\n",
            "0.12: 0.99426\n",
            "0.13: 0.99444\n",
            "0.14: 0.99456\n",
            "0.15: 0.99469\n",
            "0.16: 0.99479\n",
            "0.17: 0.99485\n",
            "0.18: 0.99489\n",
            "0.19: 0.99496\n",
            "0.20: 0.99500\n",
            "0.21: 0.99508\n",
            "0.22: 0.99512\n",
            "0.23: 0.99514\n",
            "0.24: 0.99518\n",
            "0.25: 0.99523\n",
            "0.26: 0.99528\n",
            "0.27: 0.99529\n",
            "0.28: 0.99531\n",
            "0.29: 0.99534\n",
            "0.30: 0.99534\n",
            "0.31: 0.99538\n",
            "0.32: 0.99541\n",
            "0.33: 0.99543\n",
            "0.34: 0.99544\n",
            "0.35: 0.99546\n",
            "0.36: 0.99546\n",
            "0.37: 0.99546\n",
            "0.38: 0.99546\n",
            "0.39: 0.99546\n",
            "0.40: 0.99545\n",
            "0.41: 0.99545\n",
            "0.42: 0.99546 best\n",
            "0.43: 0.99545\n",
            "0.44: 0.99543\n",
            "0.45: 0.99540\n",
            "0.46: 0.99541\n",
            "0.47: 0.99540\n",
            "0.48: 0.99538\n",
            "0.49: 0.99534\n",
            "0.50: 0.99532\n",
            "0.51: 0.99528\n",
            "0.52: 0.99529\n",
            "0.53: 0.99526\n",
            "0.54: 0.99523\n",
            "0.55: 0.99519\n",
            "0.56: 0.99515\n",
            "0.57: 0.99512\n",
            "0.58: 0.99509\n",
            "0.59: 0.99506\n",
            "0.60: 0.99504\n",
            "0.61: 0.99501\n",
            "0.62: 0.99496\n",
            "0.63: 0.99492\n",
            "0.64: 0.99487\n",
            "0.65: 0.99483\n",
            "0.66: 0.99478\n",
            "0.67: 0.99475\n",
            "0.68: 0.99468\n",
            "0.69: 0.99464\n",
            "0.70: 0.99457\n",
            "0.71: 0.99453\n",
            "0.72: 0.99444\n",
            "0.73: 0.99439\n",
            "0.74: 0.99433\n",
            "0.75: 0.99425\n",
            "0.76: 0.99420\n",
            "0.77: 0.99410\n",
            "0.78: 0.99402\n",
            "0.79: 0.99396\n",
            "0.80: 0.99385\n",
            "0.81: 0.99377\n",
            "0.82: 0.99364\n",
            "0.83: 0.99351\n",
            "0.84: 0.99336\n",
            "0.85: 0.99326\n",
            "0.86: 0.99307\n",
            "0.87: 0.99289\n",
            "0.88: 0.99273\n",
            "0.89: 0.99251\n",
            "0.90: 0.99228\n",
            "0.91: 0.99200\n",
            "0.92: 0.99167\n",
            "0.93: 0.99132\n",
            "0.94: 0.99085\n",
            "0.95: 0.99032\n",
            "0.96: 0.98978\n",
            "0.97: 0.98895\n",
            "0.98: 0.98779\n",
            "0.99: 0.98568\n",
            "1.00: 0.97760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f_score(model, val_loader, device, threshhold=0.9):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    gts = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
        "            outputs = F.sigmoid(model(batch[0].to(device))) > threshhold\n",
        "            preds += list(outputs.cpu().numpy())\n",
        "            gts += list(batch[1].cpu().numpy())\n",
        "    from sklearn.metrics import f1_score\n",
        "    del batch\n",
        "    utils.empty_cache()\n",
        "    return f1_score(gts, preds, average=None, zero_division=1.0)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "all_set = PianoDataset(k=5)\n",
        "all_dl = DataLoader(all_set,\n",
        "                    batch_size=32,\n",
        "                    num_workers=12,\n",
        "                    )\n",
        "model = PianoModelSmallSelf(k=5)\n",
        "model.load_state_dict(torch.load(\"best_model\"), strict=True)\n",
        "model = model.to(device).eval()\n",
        "f_scores_per_class = f_score(model, all_dl, device)"
      ],
      "metadata": {
        "id": "d-k9xPAKZwWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.bar(list(range(88)), f_scores_per_class)\n",
        "plt.title(\"F1-score\")\n",
        "plt.xlabel(\"Key on a piano\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "T-p-fUwQZ0Og",
        "outputId": "ef52f55d-3d24-4c55-da14-a0a7d5602534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    }
  ]
}